# Random practice

All the things I made to get a better understanding of how neural networks work. They all have plenty of comments that I made when I was trying to wrap my head around it

## [one_neuron_back_prop.py](https://github.com/michael-lesirge/neural-network/blob/main/random_practice/one_neuron_back_prop.py)
<p>Animation of one neuron using backpropagation to find line</p>
<img src="https://github.com/michael-lesirge/neural-network/blob/main/random_practice/one_neuron_back_prop.gif" alt="One neuron learning line">

## [gradient_decent_one_neuron.py](https://github.com/michael-lesirge/neural-network/blob/main/random_practice/gradient_decent_one_layer.py) and [gradient_decent_one_layer.py](https://github.com/michael-lesirge/neural-network/blob/main/random_practice/gradient_decent_one_layer.py)
<p>Output and loss of neural network layer</p>
<img src="https://github.com/michael-lesirge/neural-network/assets/100492377/dbbc7672-64c7-4845-ad1b-26cf0465e1bd" alt="One neuron matching output" width="400"/>
<img src="https://github.com/michael-lesirge/neural-network/assets/100492377/39461f47-a02b-48eb-96a1-057d62843b82" alt="One neuron matching loss" width="400"/>
